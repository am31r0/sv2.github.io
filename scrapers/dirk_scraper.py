#!/usr/bin/env python3
"""
dirk_full_scraper.py

Scraper voor Dirk.nl producten via GraphQL API.
- Loopt webGroupId 1..150 af
- Haalt alle producten per webGroupId (geen paginatie nodig)
- Neemt alle gevraagde details mee, incl. categoryLabel (webgroup uit de API)
- Kan met 'n' tijdens runtime stoppen en data netjes opslaan
"""

import requests
import json
import os
import time
import random
import threading
from datetime import datetime
from tqdm import tqdm

# ===== CONFIG =====
URL = "https://web-dirk-gateway.detailresult.nl/graphql"
HEADERS = {"User-Agent": "Mozilla/5.0", "Content-Type": "application/json"}

STORE_ID = 66
MAX_RETRIES = 3
MIN_DELAY = 2.0
MAX_DELAY = 5.0

BATCH_SIZE = 1000

RANGE_START = 1
RANGE_END = 150
# ==================

QUERY_TEMPLATE = """
query {
  listWebGroupProducts(webGroupId: %d) {
    productAssortment(storeId: %d) {
      productId
      normalPrice
      offerPrice
      productOffer {
        textPriceSign
        endDate
        startDate
      }
      productInformation {
        headerText
        packaging
        brand
        image
        department
        webgroup
      }
    }
  }
}
"""

STOP_REQUESTED = False

def listen_for_stop():
    """Thread die 'n' checkt en het proces kan stoppen."""
    global STOP_REQUESTED
    while True:
        key = input().strip().lower()
        if key == "n":
            STOP_REQUESTED = True
            print("\n[INFO] Stop-signaal ontvangen. Afronden...")

def fetch_webgroup(wgid):
    """Haal producten van één webGroupId op."""
    query = QUERY_TEMPLATE % (wgid, STORE_ID)
    payload = {"query": query, "variables": {}}
    for attempt in range(MAX_RETRIES):
        try:
            r = requests.post(URL, headers=HEADERS, json=payload, timeout=30)
            r.raise_for_status()
            data = r.json()
            assortment = (
                data.get("data", {})
                .get("listWebGroupProducts", {})
                .get("productAssortment", [])
            )
            return [p for p in assortment if p]
        except Exception as e:
            if attempt == MAX_RETRIES - 1:
                print(f"[ERROR] webGroupId={wgid} -> {e}")
                return []
            time.sleep(1.5 * (attempt + 1))
    return []

def scrape_webgroup(wgid):
    """Scrape alle producten uit een webGroupId."""
    products = []
    group_data = fetch_webgroup(wgid)

    for p in group_data:
        info = p.get("productInformation", {}) or {}
        offer = p.get("productOffer") or {}

        normal_price = p.get("normalPrice")
        offer_price = p.get("offerPrice")

        # Alleen promoPrice tonen als er een aanbieding is
        promo_price = offer_price if offer_price and offer_price < normal_price else None

        products.append({
            "id": p.get("productId"),
            "title": info.get("headerText"),
            "category": info.get("webgroup") or info.get("department"),
            "price": normal_price,
            "promoPrice": promo_price,
            "priceLabel": offer.get("textPriceSign"),
            "price_per_unit": None,  # Dirk geeft geen prijs per eenheid terug
            "unit": info.get("packaging"),
            "image": info.get("image"),
            "beschikbaar": True,
            "promoStart": offer.get("startDate"),   # ✅ toegevoegd
            "promoEnd": offer.get("endDate"),       # ✅ toegevoegd
            "link": f"https://www.dirk.nl/product/{p.get('productId')}",
            "brand": info.get("brand"),
            "webgroupId": wgid,
            "categoryLabel": info.get("webgroup"),
        })
    return products


def save_batch(products, batch_num, run_id):
    """Sla tussentijdse batch op in de juiste map."""
    OUTPUT_DIR = "dev/store_database"
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    outfile = os.path.join(OUTPUT_DIR, "dirk.json")
    with open(outfile, "w", encoding="utf-8") as f:
        json.dump(products, f, ensure_ascii=False, indent=2)
    print(f"[SAVED] Batch {batch_num} -> {len(products)} producten (naar {outfile})")

def main():
    global STOP_REQUESTED
    run_id = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    all_products = []
    batch_num = 1

    threading.Thread(target=listen_for_stop, daemon=True).start()

    print("=== Dirk.nl Product Scraper ===")
    for wgid in tqdm(range(RANGE_START, RANGE_END + 1), desc="Scraping webGroups", unit="group"):
        if STOP_REQUESTED:
            break
        group_products = scrape_webgroup(wgid)
        if group_products:
            print(
                f"[FOUND] webGroupId={wgid} -> {len(group_products)} producten "
                f"(voorbeeld: {group_products[0].get('name')})"
            )
            all_products.extend(group_products)
            if len(all_products) >= batch_num * BATCH_SIZE:
                save_batch(all_products, batch_num, run_id)
                batch_num += 1
        time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

    # === EINDBESTAND ===
    OUTPUT_DIR = "dev/store_database"
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    outfile = os.path.join(OUTPUT_DIR, "dirk.json")
    with open(outfile, "w", encoding="utf-8") as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)

    print("\n=== DONE ===")
    print(f"Totaal gevonden producten: {len(all_products)}")
    print(f"Full dataset opgeslagen in: {outfile}")

if __name__ == "__main__":
    main()
